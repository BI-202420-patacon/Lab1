{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 1 - Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook tiene los siguientes elementos: \n",
    "1. Cargue de los datos.\n",
    "\n",
    "2. Entendimiento de los datos: Describir las características más relevantes de los datos y todo el perfilamiento de datos, incluir el análisis de calidad de datos y hacer una preselección de las variables más importantes para la etapa de modelado.\n",
    "\n",
    "3. Preparación de datos: Solucionar los problemas de calidad de datos previamente identificados que afecten el modelo a construir. Además, debe aplicar todos los proceso de preprocesamiento de datos necesarios para la construcción del modelo de regresión.\n",
    "\n",
    "4. Modelado: Utilizando las variables previamente seleccionadas, construir un modelo de regresión que estime la variable objetivo con el menor error posible.\n",
    "\n",
    "5. Evaluación cuantitativa: A partir de las métricas seleccionadas para evaluar y seleccionar el mejor modelo, explicar el resultado obtenido desde el punto de vista cuantitativo. Contestar a la pregunta: ¿Su equipo recomienda utilizar en producción el modelo de regresión para estimar los tiempos? ¿Por qué? En caso de no recomendar el uso del modelo, ¿qué recomendaciones haría para continuar iterando con el objetivo de la construcción de un mejor modelo?\n",
    "\n",
    "6. Evaluación cualitativa: Debe estar compuesta de dos partes:\n",
    "- Validación de supuestos: Realizar los ajustes necesarios para que el modelo cumpla con los supuestos necesarios para la inferencia estadística con regresiones.\n",
    "- Interpretación de los coeficientes: Realizar la interpretación de los coeficientes de la regresión, identificando las variables más relevantes para la estimación y cómo afectan la variable objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendimiento del negocio:\n",
    "El caso de estudio es de un hospital que haciendo uso de la metodología KTAS quiere solicitar un modelo que pueda pronosticar el tiempo de duración de una persona en el hospital con base en sus condiciones de llegada.\n",
    "### Enfoque Analítico:\n",
    "En este laboratorio vamos a hacer un modelo predictivo usando un aprendizaje supervisado y un modelo de regresión lineal para hacer uso de las condiciones de llegada de los pacientes y predecir la duración de su estancia en el hospital en minutos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import contractions\n",
    "import re, string, unicodedata\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"./data/Regresión_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entendimiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se nos proporcionaron dos CSV, uno para entrenar el modelo y otro para probarlo, al analizar los datos dados en el CSV de entrenamiento seguimos los siguientes pasos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el CSV hay datos de 1000 pacientes, con 23 características independientes.\n",
    "\n",
    "Se puede ver que un 75% de los valores de la variable objetivo \"Duracion_Estancia_Min\" están sobre los 620 minutos (10 horas y 20 minutos), sin embargo, hay datos que alcanzan hasta los 709,510 minutos (1 año y 127 días)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ver los atributos de cada una de las filas agrupamos las características de la siguiente manera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Category |Fields|\n",
    "|----------|------|\n",
    "| Demografía |Sexo, Edad, Grupo|\n",
    "| Accidente |Modo Llegada, Lesión, Queja, Principal|\n",
    "| Signos Vitales |Estado Mental, SBP, DBP, HR, RR, BT, Saturación, Dolor|\n",
    "| Diagnósticos |dolor NRS, KTAS enfermera, Diagnóstico En Urgencias, Disposición, KTAS experto, Duración_Estancia_Min, Duración_KTAS_Min, Error_Triaje|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que nos dice es que una fila de los datos que tiene información de un paciente se compone de su demografía, accidente, los signos vitales y el diagnóstico que le dio en el ala de urgencias.\n",
    "Entré las características más relevantes se encuentran:\n",
    "- Los signos vitales: Estado mental, presión Arterial Sistólica, Presión Arterial Diastólica, Frecuencia cardíaca, Frecuencia Respiratoria, Temperatura Corporal y Saturación de Oxígeno.\n",
    "- La edad de los pacientes.\n",
    "- El Triaje realizado por la enfermera y los expertos.\n",
    "- El modo en el que llevan al hospital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza un análisis de completitud en el que podemos ver que todos los datos están completos, cuentan con 1000 registros, menos los signos vitales. El más destacado es la saturación, que un 50% de los datos no cuenta con este valor. El resto de los signos vitales no supera el 20% de faltantes. Creemos que la toma de estos signos está relacionada con la gravedad del estado del paciente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.isnull().sum() / datos.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de la variable dolor_NRS están completos, no hay nulos, pero casi el 44% de los datos está marcado con #BOÞ!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"dolor_NRS\"].value_counts()/datos.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Duracion_KTAS_Min se encuentra guardada como object y no como float64. Se decide realizar este cambio para poder continuar con el entendimiento de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"Duracion_KTAS_Min\"] = datos[\"Duracion_KTAS_Min\"].str.replace(',', '.').astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Búsqueda de relaciones con la variable objetivo\n",
    "\n",
    "Si se logra visualizar o cuantificar altas correlaciones entre las variables de entrada y la variable objetivo, se podrán soportar las decisiones del experto con base en la evidencia.\n",
    "\n",
    "Para fines prácticos se extrae en una lista todas las variables numéricas que se pueden procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas = [\"Grupo\", \"Sexo\", \"Edad\",  \"Modo_Llegada\", \"Lesion\", \"Estado_Mental\", \"Dolor\",\"KTAS_enfermera\", \"SBP\", \"DBP\", \"HR\", \"RR\", \"BT\", \"Saturacion\", \"Disposicion\", \"KTAS_experto\", \"Duracion_KTAS_Min\", \"Duracion_Estancia_Min\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que existen una gran brecha entre la duración de la estancia, que el 75% está por debajo de 700 y después de esos se dispara, tomamos la decisión de no contar con esos datos para poder seguir con el entendimiento. Como se puede ver a continuación. Los datos crecen de forma razonable hasta cierto punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.sort(datos[\"Duracion_Estancia_Min\"]+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(datos[\"Duracion_Estancia_Min\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para continuar con el entendimiento se decide tomar el percentil 0.8 de los datos, lo que equivale a todas las filas que el tiempo de su estancia sea menor a 858 minutos, lo que nos da unos datos menos dispersos y manejables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = datos[\"Duracion_Estancia_Min\"].quantile(0.8)\n",
    "datos_recorte =datos[datos[\"Duracion_Estancia_Min\"]<=val]\n",
    "sns.boxplot(datos_recorte[\"Duracion_Estancia_Min\"])\n",
    "print(datos_recorte.shape)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos una gráfica scatter por cada variable numérica para tratar de ver tendencias de comportamiento de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in numericas:\n",
    "    sns.pairplot(datos_recorte.sample(frac=0.2), height=3, y_vars=\"Duracion_Estancia_Min\", x_vars=variable, kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo más notable de estos gráficos y que usaremos luego, es una posible distribución logarítmica normal de la variable de Duracion_KTAS_Min contra Duracion_Estancia_Min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos con todos los datos y su relación con la variable objetivo, se puede apreciar que ninguna de las variables, obviando \"Grupo\", superan el 5% de coeficiente de correlación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    datos[numericas].corr(),\n",
    "    cmap=cmap,\n",
    "    vmin=-1, vmax=1,\n",
    "    annot=True\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, haciendo uso del recorte, las correlaciones aumentan significativamente, sin embargo, siguen habiendo correlaciones bastante poco significativas (no superiores al 25%).\n",
    "\n",
    "Las variables que más relaciones tienen son Grupo: 40%, Lesion: 16%, KTAS_experto: 24%, edad: 21%, la disposicion: 18% y la duración del KTAS: 20%. Los signos vitales son muy dispersos y podemos optar por unirlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    datos_recorte[numericas].corr(),\n",
    "    cmap=cmap,\n",
    "    vmin=-1, vmax=1,\n",
    "    annot=True\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la preselección de los datos elegimos las variables más relevantes con relación a la objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidatas = [\"Grupo\", \"Lesion\",\"KTAS_experto\",\"Disposicion\",  \"Edad\", \"Duracion_KTAS_Min\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problemas de calidad:\n",
    "\n",
    "Primero el dolor reportado por la enfermera, el cual tiene un 44% de sus entradas en null.\n",
    "Nos dimos cuenta de que estas se correspondían cuando el paciente no tenía dolor, por lo que las asignamos a 0. Esto tenía una excepción en 2 entradas, las cuales registraban que el paciente tenía dolor, pero no especificaba cuál, por lo que le asignamos la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calida_dolor_NRS(registro):\n",
    "    if registro[\"dolor_NRS\"] == \"#BOÞ!\" and registro[\"Dolor\"] == 0:\n",
    "        return 0\n",
    "    elif registro[\"dolor_NRS\"] == \"#BOÞ!\" and registro[\"Dolor\"] != 0:\n",
    "        return registro[\"dolor_NRS\"]\n",
    "    return int(registro[\"dolor_NRS\"])\n",
    "datos['dolor_NRS'] = datos.apply(calida_dolor_NRS, axis=1)\n",
    "\n",
    "mean = datos[datos[\"dolor_NRS\"] != \"#BOÞ!\" ]['dolor_NRS'].mean()\n",
    "datos[\"dolor_NRS\"] = datos[\"dolor_NRS\"].apply(lambda x: round(mean) if x == \"#BOÞ!\" else x)\n",
    "datos[\"dolor_NRS\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después usamos la técnica del One-Hot, para estandarizar el Sexo y la Lesión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"Sexo_stan\"] = datos[\"Sexo\"].apply(lambda x: 0 if x == 2 else x)\n",
    "datos[\"Sexo_stan\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"Lesion_stan\"]=datos[\"Lesion\"].apply(lambda x:0 if x == 2 else x)\n",
    "datos[\"Lesion_stan\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hicimos una investigación para agrupar los signos vitales en esta y llegamos al Early Warning Score (EWS) es una herramienta clínica utilizada para identificar a los pacientes que están en riesgo de deterioro. El cálculo del EWS generalmente se basa en una serie de parámetros fisiológicos como la frecuencia cardíaca, la presión arterial, la temperatura, la frecuencia respiratoria, y el nivel de conciencia, la calculamos y la agregamos a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularEWS(registro):\n",
    "    total = 0\n",
    "    \n",
    "    # Frecuencia respiratoria (RR)\n",
    "    if registro.get('RR', np.nan) <= 8:\n",
    "        total += 2\n",
    "    elif 9 <= registro.get('RR', np.nan) <= 14:\n",
    "        total += 0\n",
    "    elif 15 <= registro.get('RR', np.nan) <= 20:\n",
    "        total += 1\n",
    "    elif 21 <= registro.get('RR', np.nan) <= 29:\n",
    "        total += 2\n",
    "    elif registro.get('RR', np.nan) >= 30:\n",
    "        total += 3\n",
    "    \n",
    "    # Presión arterial sistólica (SBP)\n",
    "    if registro.get('SBP', np.nan) <= 70:\n",
    "        total += 3\n",
    "    elif 71 <= registro.get('SBP', np.nan) <= 80:\n",
    "        total += 2\n",
    "    elif 81 <= registro.get('SBP', np.nan) <= 100:\n",
    "        total += 1\n",
    "    elif 101 <= registro.get('SBP', np.nan) <= 199:\n",
    "        total += 0\n",
    "    elif registro.get('SBP', np.nan) >= 200:\n",
    "        total += 2\n",
    "\n",
    "    # Frecuencia cardíaca (HR)\n",
    "    if registro.get('HR', np.nan) <= 40:\n",
    "        total += 2\n",
    "    elif 41 <= registro.get('HR', np.nan) <= 50:\n",
    "        total += 1\n",
    "    elif 51 <= registro.get('HR', np.nan) <= 100:\n",
    "        total += 0\n",
    "    elif 101 <= registro.get('HR', np.nan) <= 110:\n",
    "        total += 1\n",
    "    elif 111 <= registro.get('HR', np.nan) <= 129:\n",
    "        total += 2\n",
    "    elif registro.get('HR', np.nan) >= 130:\n",
    "        total += 3\n",
    "\n",
    "    # Temperatura corporal (BT)\n",
    "    if registro.get('BT', np.nan) < 35.0:\n",
    "        total += 2\n",
    "    elif 35.0 <= registro.get('BT', np.nan) <= 38.4:\n",
    "        total += 0\n",
    "    elif registro.get('BT', np.nan) >= 38.5:\n",
    "        total += 2\n",
    "\n",
    "    # Saturación de oxígeno (Saturacion)\n",
    "    if registro.get('Saturacion', np.nan) <= 91:\n",
    "        total += 3\n",
    "    elif 92 <= registro.get('Saturacion', np.nan) <= 93:\n",
    "        total += 2\n",
    "    elif 94 <= registro.get('Saturacion', np.nan) <= 95:\n",
    "        total += 1\n",
    "\n",
    "    # Nivel de conciencia\n",
    "    if registro.get('Estado_Mental', np.nan) == 1:\n",
    "        total += 0\n",
    "    elif registro.get('Estado_Mental', np.nan) == 2:\n",
    "        total += 1\n",
    "    elif registro.get('Estado_Mental', np.nan) == 3:\n",
    "        total += 2\n",
    "    elif registro.get('Estado_Mental', np.nan) == 4:\n",
    "        total += 3\n",
    "\n",
    "    return total\n",
    "\n",
    "datos['EWS'] = datos.apply(calcularEWS, axis=1)\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, intentamos linealizar la \"Duracion_KTAS_Min\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"Dur_KTAS_lin\"] = np.log(datos[\"Duracion_KTAS_Min\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas = [\"Grupo\", \"Sexo\", \"Edad\",  \"Modo_Llegada\", \"Lesion\",\"Lesion_stan\", \"Estado_Mental\", \"Dolor\",\"KTAS_enfermera\", \"EWS\", \"Disposicion\", \"KTAS_experto\", \"Duracion_KTAS_Min\",\"Dur_KTAS_lin\", \"Duracion_Estancia_Min\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = datos[\"Duracion_Estancia_Min\"].quantile(0.8)\n",
    "datos_recorte =datos[datos[\"Duracion_Estancia_Min\"]<=val]\n",
    "plt.figure(figsize=(12, 10))\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    datos_recorte[numericas].corr(),\n",
    "    cmap=cmap,\n",
    "    vmin=-1, vmax=1,\n",
    "    annot=True\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando este nuevo heatmap tenemos las siguientes variables candidatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidatas = [\"Grupo\", \n",
    "              \"Edad\", \n",
    "              \"Disposicion\",\n",
    "              \"Lesion_stan\",\n",
    "              \"KTAS_experto\",\n",
    "              \"EWS\",\n",
    "              \"Duracion_KTAS_Min\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    datos_recorte[candidatas+[\"Duracion_Estancia_Min\"]].corr(),\n",
    "    cmap=cmap,\n",
    "    vmin=-1, vmax=1,\n",
    "    annot=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos una limpieza de duplicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan los registros totalmente duplicados\n",
    "datos = datos.dropna(subset=[\"Duracion_Estancia_Min\"]+candidatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = datos.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se hay duplicados totales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[[\"Duracion_Estancia_Min\"]+candidatas].isnull().sum() / datos.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicados parciales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.loc[datos.duplicated(subset=candidatas, keep=False)][[\"Duracion_Estancia_Min\"]+candidatas].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = datos.loc[datos.duplicated(subset=candidatas, keep=False)].shape[0]\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates: {(duplicated_rows/total_rows)*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.loc[datos.duplicated(subset=candidatas+[\"Duracion_Estancia_Min\"], keep=False)].tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = datos.loc[datos.duplicated(subset=candidatas+[\"Duracion_Estancia_Min\"], keep=False)].shape[0]\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates: {(duplicated_rows/total_rows)*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.drop_duplicates(subset=candidatas, inplace=True)\n",
    "datos.drop_duplicates(subset=candidatas+[\"Duracion_Estancia_Min\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hay dos escenarios a analizar:\n",
    "1. Existe 94 registros o un ~9.4% de registros con variables de entrada duplicadas, con variable objetivo diferente. Una cantidad es un poco preocupante que requieren ser limpiados para no confundir al modelo.\n",
    "\n",
    "2. Al incluir la variable objetivo dentro del análisis de duplicados, se obtiene el ~6.7% registros duplicados adicionales. Esto es un problema potencial que obligaría al algoritmo de optimización a enfocarse más en aquellos registros duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = datos[\"Duracion_Estancia_Min\"].quantile(0.75)\n",
    "datos_recorte =datos[datos[\"Duracion_Estancia_Min\"]<=val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Particionamiento del conjunto de datos en entrenamiento y prueba\n",
    "\n",
    "Se desea construir un modelo que se ajuste bien a los datos de entrenamiento, pero que además se comporte de forma similar con datos previamente desconocidos.\n",
    "\n",
    "En esta parte se dividen los datos en dos conjuntos, prueba y entrenamiento. El conjunto de prueba corresponderá al 30% de los datos limpiados previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(datos_recorte[candidatas], datos_recorte[\"Duracion_Estancia_Min\"], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelado\n",
    "\n",
    "Utilizando las variables previamente seleccionadas, construir un modelo de regresión que estime la variable objetivo con el menor error posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluación cualitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"columns\": candidatas, \"coef\": regression.coef_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, len(candidatas), sharey=True, figsize=(20, 5), layout=\"constrained\")\n",
    "\n",
    "for i in range(len(candidatas)):\n",
    "    col = candidatas[i]\n",
    "    x = X_train[col]\n",
    "    m = regression.coef_[i]\n",
    "    b = regression.intercept_\n",
    "\n",
    "    axs[i].plot(x, y_train, \"o\", alpha=0.1)\n",
    "    axs[i].plot(x, x * m + b)\n",
    "    axs[i].set_title(col)\n",
    "print(\"Train:\", np.sqrt(mean_squared_error(y_train, regression.predict(X_train))))\n",
    "print(\"Test:\", np.sqrt(mean_squared_error(y_test, regression.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "sns.boxplot(x=y_test, showmeans=True, orient=\"h\")\n",
    "plt.title(\"Valor real de $\\t{Duracion Estancia Min}$ en el conjunto de prueba\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "sns.boxplot(x=abs(y_test - regression.predict(X_test)), showmeans=True, orient=\"h\")\n",
    "plt.title(\"|Valor real - Valor estimado|/Valor estimado de $\\t{Tiempo}$\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"columns\": candidatas, \"coef\": regression.coef_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Validación de supuestos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre los supuestos del análisis era que:\n",
    "- Debido a la convención del KTAS (entre más leve la emergencia mayor el KTAS), este tendría un coeficiente negativo con respecto a la duración del paciente en el hospital. Además de esto, podemos ver que el KTAS dado por el experto es un poco más significativo y útil que el dado por la enfermera.\n",
    "\n",
    "- Entre más edad tiene el paciente mayor va a ser el tiempo que dura en el hospital.\n",
    "\n",
    "- Entre peor sea la disposición de llegada del paciente, también aumenta su duración.\n",
    "\n",
    "- Entre peor sean sus signos vitales (EWS) más tiempo tomaría su visita al hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Interpretación de los coeficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo al que llegamos tiene un error cuadrático medio de alrededor de 130 minutos, es decir que hay un margen de error de 130 minutos entre lo que predice el modelo y el valor real que va a demorarse el paciente. Por esta razón, consideramos que el modelo no está listo para ser usado y no es aún una herramienta fiable para calcular la duración de la estancia de los pacientes.\n",
    "\n",
    "En cuanto a los coeficientes podemos ver el peso que tienen cada una de estas variables en el tiempo de estadía del paciente:\n",
    "\n",
    "- Grupo, en la cual un \"aumento\" de grupo implica un aumento de 118 minutos.\n",
    "\n",
    "- Edad, cada año adicional del paciente implica 1 minuto adicional de espera, lo cual puede no verse como mucho, pero teniendo en cuenta que el 75% de los pacientes están sobre los 70 años, esto vuelve a la edad muy significativa.\n",
    "\n",
    "- Disposición, el resultado de su proceso de urgencias significa 6 minutos adicionales, entre peor se considere (6 minutos para un alta a domicilio, mientras que la cirugía implica 42 minutos adicionales).\n",
    "\n",
    "- Lesión, que el paciente tenga o no una lesión, implica que alrededor de 54 minutos de espera adicional.\n",
    "\n",
    "- KTAS experto, cada aumento en la escala de KTAS implica menos severidad en la emergencia, lo que se traduce en 34 minutos menos de duracion en el hospital.\n",
    "\n",
    "- EWS, el EWS es un compilado de los signos vitales y revisa si están en orden o son una, existe una amenaza para la salud del paciente, cada amenaza puede subir entre 1 y 3 puntos el EWS y cada uno de esos puntos implica 3 minutos más en la duracion.\n",
    "\n",
    "- Duración KTAS en minutos, cada minuto de evaluación KTAS implica 40 segundos de duración adicional en la consulta. (va de 0 a 20, entonces a lo sumo implica 13.6 minutos adicionales)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la exportación del modelo definimos funciones de trasnformación que son los pasos que deben seguir los datos de entreada para que el modelo prueda procesarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la funcion de transformación objetivos de elijen las variables que se aplicarán en el modelo de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "def objetivos(X):\n",
    "    X = X.copy()\n",
    "    candidatas = [\"grupo\", \"edad\", \"disposicion\", \"lesion_stan\",\"ktas_experto\",\"ews\", \"duracion_ktas_min\"]\n",
    "    return X[candidatas]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la funcion de transformación limpieza se comprueba el estado de las variables de interes:\n",
    "- Valores nulos: De serlo se rellena con el valor mas frecuente segun el entrenamiento\n",
    "- Tipos de datos: Debes ser de tipo numerico int o float\n",
    "\n",
    "Posterior se aplica la estarización mencioanda en la preparación de los datos.\n",
    "\n",
    "Por ultimo se ponen todas las llaves un minuscula para evitar errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def limpieza(X):\n",
    "    X.copy()\n",
    "    \n",
    "    X.columns = [col.lower() for col in X.columns]\n",
    "    \n",
    "    X['grupo'] = X['grupo'].fillna(0).astype(int)\n",
    "    \n",
    "    X['ktas_experto'] = X['ktas_experto'].fillna(0).astype(int)\n",
    "    \n",
    "    X['edad'] = X['edad'].fillna(0).astype(int)\n",
    "    \n",
    "    X['disposicion'] = X['disposicion'].fillna(0).astype(int)\n",
    "    \n",
    "    X['lesion'] = X['lesion'].fillna(0).astype(int)\n",
    "    X[\"lesion_stan\"] = X[\"lesion\"].apply(lambda x:0 if x == 2 else x)\n",
    "        \n",
    "    if X[\"duracion_ktas_min\"].dtype == 'object': \n",
    "        X[\"duracion_ktas_min\"] = X[\"duracion_ktas_min\"].str.replace(',', '.').astype('float64')\n",
    "    elif X[\"duracion_ktas_min\"].dtype not in ['float64', 'int64']:  \n",
    "        X[\"duracion_ktas_min\"] = pd.to_numeric(X[\"duracion_ktas_min\"], errors='coerce').fillna(0)\n",
    "    \n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrupan las variables como signos vitales en un paso adicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_EWS_pipe(registro):\n",
    "    total = 0\n",
    "    \n",
    "    # Frecuencia respiratoria (rr)\n",
    "    if registro.get('rr', np.nan) <= 8:\n",
    "        total += 2\n",
    "    elif 9 <= registro.get('rr', np.nan) <= 14:\n",
    "        total += 0\n",
    "    elif 15 <= registro.get('rr', np.nan) <= 20:\n",
    "        total += 1\n",
    "    elif 21 <= registro.get('rr', np.nan) <= 29:\n",
    "        total += 2\n",
    "    elif registro.get('rr', np.nan) >= 30:\n",
    "        total += 3\n",
    "    \n",
    "    # Presión arterial sistólica (SBP)\n",
    "    if registro.get('sbp', np.nan) <= 70:\n",
    "        total += 3\n",
    "    elif 71 <= registro.get('sbp', np.nan) <= 80:\n",
    "        total += 2\n",
    "    elif 81 <= registro.get('sbp', np.nan) <= 100:\n",
    "        total += 1\n",
    "    elif 101 <= registro.get('sbp', np.nan) <= 199:\n",
    "        total += 0\n",
    "    elif registro.get('sbp', np.nan) >= 200:\n",
    "        total += 2\n",
    "\n",
    "    # Frecuencia cardíaca (HR)\n",
    "    if registro.get('hr', np.nan) <= 40:\n",
    "        total += 2\n",
    "    elif 41 <= registro.get('hr', np.nan) <= 50:\n",
    "        total += 1\n",
    "    elif 51 <= registro.get('hr', np.nan) <= 100:\n",
    "        total += 0\n",
    "    elif 101 <= registro.get('hr', np.nan) <= 110:\n",
    "        total += 1\n",
    "    elif 111 <= registro.get('hr', np.nan) <= 129:\n",
    "        total += 2\n",
    "    elif registro.get('hr', np.nan) >= 130:\n",
    "        total += 3\n",
    "\n",
    "    # Temperatura corporal (BT)\n",
    "    if registro.get('bt', np.nan) < 35.0:\n",
    "        total += 2\n",
    "    elif 35.0 <= registro.get('bt', np.nan) <= 38.4:\n",
    "        total += 0\n",
    "    elif registro.get('bt', np.nan) >= 38.5:\n",
    "        total += 2\n",
    "\n",
    "    # Saturación de oxígeno (Saturacion)\n",
    "    if registro.get('saturacion', np.nan) <= 91:\n",
    "        total += 3\n",
    "    elif 92 <= registro.get('saturacion', np.nan) <= 93:\n",
    "        total += 2\n",
    "    elif 94 <= registro.get('saturacion', np.nan) <= 95:\n",
    "        total += 1\n",
    "\n",
    "    # Nivel de conciencia\n",
    "    if registro.get('estado_mental', np.nan) == 1:\n",
    "        total += 0\n",
    "    elif registro.get('estado_mental', np.nan) == 2:\n",
    "        total += 1\n",
    "    elif registro.get('estado_mental', np.nan) == 3:\n",
    "        total += 2\n",
    "    elif registro.get('estado_mental', np.nan) == 4:\n",
    "        total += 3\n",
    "\n",
    "    return total\n",
    "\n",
    "def agrupar_signos_vitales(X):\n",
    "        X.copy()\n",
    "        X['ews'] = X.apply(calcular_EWS_pipe, axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ordenan las intrucciones en el pipe line. La ultima instrucción \"Regresión\" corresponde a la linealización de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(datos_recorte, datos_recorte[\"Duracion_Estancia_Min\"], test_size=0.3, random_state=1)\n",
    "\n",
    "# Crear el pipeline\n",
    "pipe = Pipeline([\n",
    "    ('limpieza', FunctionTransformer(limpieza, validate=False)),\n",
    "    ('Agrupacion', FunctionTransformer(agrupar_signos_vitales, validate=False)),\n",
    "    ('objetivo', FunctionTransformer(objetivos, validate=False)),\n",
    "    ('Regresion', LinearRegression())\n",
    "])\n",
    "\n",
    "# Entrenar el pipeline\n",
    "pipe.fit(X_train, y_train).score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez armado y entrenado el modelo, se guarda en el archivo \"modelo_regresion_MediAlpes\" para su posterior utilización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(pipe, 'modelo_regresion_MediAlpes.pkl', compress=3)\n",
    "print(\"Modelo guardado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion se muestra un ejemplo de carga y ejecución del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidatas = [\"grupo\", \"lesion_stan\",\"ktas_experto\",\"ews\", \"duracion_ktas_min\"]\n",
    "registro_prueba = datos_recorte.sample(1)\n",
    "modelo_cargado = load('modelo_regresion_MediAlpes.pkl')\n",
    "y_pred = modelo_cargado.predict(registro_prueba)\n",
    "print(f\"Para el registro\\n{registro_prueba[candidatas]}\\n. Se estima una duración de la estancia de {y_pred} minutos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generar predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar las prediciones con el modelo generado se cargan los datos y el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_sin_etiquetas = pd.read_csv(\"./data/Regresión_validation_data.csv\")\n",
    "modelo = load('modelo_regresion_MediAlpes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica el modelo a los datos sin etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Duracion_Est_min_prediccion = modelo.predict(datos_sin_etiquetas)\n",
    "Duracion_Est_min_prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se unen las predicciones con los datos sin entiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_predicion = pd.concat([datos_sin_etiquetas, pd.Series(Duracion_Est_min_prediccion, name = \"Duracion_Estancia_Min\")], axis = 1)\n",
    "datos_predicion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guarda en un archivo los resultados de las predicciones con la llave \"Duracion_Estancia_Min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_predicion.to_csv(\"./data/Regresión_predict_data.csv\", sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['Queja_Principal'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = datos.copy()\n",
    "textos['Conteo'] = [len(x) for x in textos['Queja_Principal']]\n",
    "textos['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos['Queja_Principal']]\n",
    "textos['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos['Queja_Principal']]\n",
    "textos[[\"Queja_Principal\", \"Conteo\", \"Max\",\t\"Min\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word:  # Verifica que la palabra no sea None o una cadena vacía\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = [word.lower() for word in words if word]  # Usa comprensión de listas\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = [re.sub(r'[^\\w\\s]', '', word) for word in words if word]  # Usa comprensión de listas\n",
    "    return [word for word in new_words if word]\n",
    "\n",
    "#def replace_numbers(words):\n",
    "#    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "#    p = inflect.engine()\n",
    "#    print(words)\n",
    "#    new_words = []\n",
    "#    for word in words:\n",
    "#        if word.isdigit():\n",
    "#            new_word = p.number_to_words(word)\n",
    "#            new_words.append(new_word)\n",
    "#            print(\"if \" + new_word)\n",
    "#        else:\n",
    "#            new_words.append(word)\n",
    "#    return new_words\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))  # Usa un set para búsquedas más rápidas\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    "#   words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize_words(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['Queja_Principal'] = datos['Queja_Principal'].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['words'] = datos['Queja_Principal'].apply(lambda x: preprocessing(word_tokenize(x)))\n",
    "datos['words1']=datos['words'].apply(preprocessing)\n",
    "datos['words'] = datos['words1'].apply(lambda x: ' '.join(map(str, x)))\n",
    "datos['words'].dropna()\n",
    "\n",
    "datos.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = datos['words'],datos['Duracion_Estancia_Min']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = CountVectorizer(binary=True)\n",
    "X_dummy = dummy.fit_transform(X_data)  # X_data es tu lista de textos\n",
    "print(X_dummy.shape)\n",
    "\n",
    "# Paso 2: Convertir a DataFrame (esto es opcional si solo quieres ver los datos en formato de tabla)\n",
    "X_dense = X_dummy.toarray()\n",
    "X_df = pd.DataFrame(X_dense, columns=dummy.get_feature_names_out())\n",
    "\n",
    "# Paso 3: Aplicar TF-IDF usando TfidfTransformer\n",
    "tt = TfidfTransformer(norm='l2', use_idf=True)\n",
    "tt_matrix = tt.fit_transform(X_dummy)  # Usa X_dummy directamente\n",
    "\n",
    "# Paso 4: Convertir a una matriz densa si es necesario para inspección o para usarla en otro proceso\n",
    "tt_matrix_dense = tt_matrix.toarray()\n",
    "\n",
    "# Paso 5: Crear DataFrame con la matriz TF-IDF\n",
    "vocab = dummy.get_feature_names_out()\n",
    "tt_df = pd.DataFrame(np.round(tt_matrix_dense, 2), columns=vocab)\n",
    "tt_df[\"abdomen\"].value_counts()\n",
    "\n",
    "pd.DataFrame([df], columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir cada fila de la matriz en una lista\n",
    "rows_as_lists = np.round(tt_matrix_dense, 2).tolist()\n",
    "\n",
    "# Crear un DataFrame donde cada fila es una lista\n",
    "tt_df = pd.DataFrame(rows_as_lists)\n",
    "\n",
    "# Convertir las filas del DataFrame en una sola columna\n",
    "tt_df = pd.DataFrame({'vector_palabras': tt_df.values.tolist()})\n",
    "\n",
    "tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_vector = datos.reset_index()\n",
    "datos_vector = pd.concat([datos_vector, tt_df], axis=1)\n",
    "datos_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(datos_vector, y_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "\n",
    "# Entrenar el modelo\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\", np.sqrt(mean_squared_error(y_train, regressor.predict(X_train))))\n",
    "print(\"Test:\", np.sqrt(mean_squared_error(y_test, regressor.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "# Entrenar el modelo\n",
    "regressor.fit([np.array([0,1]), np.array([0,2])], [np.array([1,0]), np.array([2,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
